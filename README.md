# ğŸ“Š SSA Denoising Pipeline

*Paper Title*: **Denoising Stock Index Time Series via Truncated Singular Value Decomposition on Hankel Matrices**

*Description*: Implementation of **Singular Spectrum Analysis (SSA)** with integrated mean-centering for denoising financial time series. Paper for IF2123 Linear Algebra and Geometry - ITB (2025).

## ğŸ“Œ Overview

This project implements a **complete SSA pipeline** for removing noise from financial time series using **Truncated SVD (TSVD)** and **Hankel matrix embedding**.

**Key Results (IHSG 2024):**
- ğŸ¯ **86.3% MSE reduction** vs Simple Moving Average
- ğŸ¯ **12.3x smoothness improvement** vs original noisy data
- ğŸ¯ **RÂ² = 0.1005** (non-linearity preserved)
- ğŸ¯ **5 dominant components** with total 91.03% cumulative energy

**Theoretical Guarantee:** Eckart-Young-Mirsky theorem ensures optimal low-rank approximation in Frobenius norm.


## ğŸš€ Setup

### 1ï¸âƒ£ Create Virtual Environment

**Windows:**
```bash
python -m venv venv
.\venv\Scripts\activate
```

**macOS / Linux:**
```bash
python3 -m venv venv
source venv/bin/activate
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install --upgrade pip
pip install numpy scipy pandas matplotlib yfinance jupyter
```

Or use requirements.txt:
```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Run Notebook

```bash
jupyter notebook SSA_Pipeline.ipynb
```


## ğŸ”¬ SSA Pipeline Overview

### 6-Step Algorithm

```
Step 0: Mean-Centering
â”œâ”€ Compute È³ = (1/N) Î£ yâ‚œ
â””â”€ Center: á»¹â‚œ = yâ‚œ - È³

Step 1: Embedding
â”œâ”€ Create Hankel matrix X âˆˆ â„^(LÃ—K)
â””â”€ L = N/2 (window length), K = N - L + 1

Step 2: Decomposition
â”œâ”€ Compute SVD: X = UÎ£Váµ€
â””â”€ Extract singular values Ïƒâ‚ â‰¥ Ïƒâ‚‚ â‰¥ ... â‰¥ Ïƒáµ£

Step 3: Rank Selection
â”œâ”€ Cumulative energy: Eâ‚– = Î£(Ïƒáµ¢Â²) / Î£(Ïƒâ±¼Â²)
â””â”€ Select k = argmin{Eâ‚– â‰¥ Ï„}, Ï„ = 0.90

Step 4: Reconstruction
â”œâ”€ Form rank-k approximation: Xâ‚– = Î£(i=1 to k) Ïƒáµ¢ uáµ¢ váµ¢áµ€
â””â”€ Optimal in Frobenius norm (Eckart-Young theorem)

Step 5: Diagonal Averaging
â”œâ”€ Project Xâ‚– back to Hankel matrix structure
â””â”€ Recover univariate series: sâ‚œ = (1/|Dâ‚š|) Î£ Xâ‚–[i,j]

Step 6: De-centering
â””â”€ Restore mean: Åâ‚œ = sâ‚œ + È³
```

### Why This Works

**Signal vs Noise Separation:**
- **High-correlation signal** â†’ concentrates in few large Ïƒáµ¢ values
- **Low-correlation noise** â†’ spreads across many small Ïƒáµ¢ values
- **Truncation at rank k** â†’ retains signal, discards noise

**Mean-Centering Advantage:**
- Without it: Ïƒâ‚ captures DC offset (99.97%) â†’ unrealistic k=1 selection
- With it: Ïƒâ‚ captures oscillations â†’ realistic k=5 selection â†’ preserves non-linearity

**Optimality:**
The rank-k approximation Xâ‚– minimizes the Frobenius norm error over all rank-k matrices:

âˆ¥X - Xâ‚–âˆ¥_F = min_{rank(B)â‰¤k} âˆ¥X - Bâˆ¥_F = âˆš(Î£_{i=k+1}^r Ïƒáµ¢Â²)

This guarantees mathematically optimal denoising with theoretical justification via the Eckart-Young-Mirsky theorem.


## ğŸ“ Project Files

```
â”œâ”€â”€ SSA_Pipeline.ipynb      â† Main implementation notebook
â”œâ”€â”€ requirements.txt        â† Python package dependencies
â””â”€â”€ README.md               â† This file
```


## ğŸ“ Educational Value

**Core Linear Algebra Concepts:**
- **Spectral Theorem** - Eigenvalue decomposition of symmetric matrices
- **Singular Value Decomposition (SVD)** - Matrix factorization into U, Î£, Váµ€
- **Eckart-Young-Mirsky Theorem** - Optimal low-rank matrix approximation
- **Frobenius Norm** - Matrix error measurement and minimization
- **Hankel Matrix Structure** - Time-delay embedding for temporal correlation
- **Orthogonal Transformations** - Preservation of geometry through matrix operations

**Application Domain:**
- Financial time series denoising
- Signal processing with matrix decomposition
- Practical demonstration of linear algebra theory


## ğŸ“„ License

This implementation is provided for educational purposes as part of **IF2123 Aljabar Linier dan Geometri - ITB**.

**Author:** Samuelson Dharmawan Tanuraharja (13524001)